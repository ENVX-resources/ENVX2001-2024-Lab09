{
  "hash": "3d8cd243043568dff5afd46a6ccfb4a6",
  "result": {
    "markdown": "---\ntitle: \"ENVX2001 Lab 07 - Regression model development\"\n---\n\n\n\n\n:::{.callout-tip}\nPlease work on this exercise by creating your own R Markdown file.\n:::\n\n\n## Exercise 1: Bird abundance\n\nThis is the *same* dataset used in the lecture.\n\nFragmentation of forest habitat has an impact of wildlife abundance.  This study looked at the relationship between bird abundance (bird ha^-1^) and the characteristics of forest patches at 56 locations in SE Victoria.  \n\nThe predictor variables are:\n\n- `ALT` Altitude (m) \n- `YR.ISOL` Year when the patch was isolated (years) \n-\t`GRAZE` Grazing (coded 1-5 which is light to heavy) \n-\t`AREA` Patch area (ha) \n-\t`DIST` Distance to nearest patch (km) \n-\t`LDIST` Distance to largest patch (km) \n\nImport the data from the \"Loyn\" tab in the MS Excel file.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\nloyn <- read_xlsx(\"mlr.xlsx\", \"Loyn\")\n```\n:::\n\n\n\nOften, the first step in model development is to examine the data.  This is a good way to get a feel for the data and to identify any issues that may need to be addressed. In this case, we will examine the data using histograms and a correlation matrix.\n\n### Histograms\n\nThere are a breadth of ways to create histograms in R. In each tab below you will find some different ways to create the same plot outputs.\n\n:::{.panel-tabset}\n\n## `hist()`\n\nThis is a straightforward way to create multiple histograms with `hist()`. The `par()` function is used to arrange the plots on the page.  The `mfrow` argument specifies the number of rows and columns of plots. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(3,3))\nhist(loyn$ABUND)\nhist(loyn$ALT)\nhist(loyn$YR.ISOL)\nhist(loyn$GRAZE)\nhist(loyn$AREA)\nhist(loyn$DIST)\nhist(loyn$LDIST)\npar(mfrow=c(1,1))\n```\n\n::: {.cell-output-display}\n![](template_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## `hist.data.frame()` from `Hmisc`\n\nThe `Hmisc` package provides a function `hist.data.frame()` that can be used to create multiple histograms, which can be called by simply using `hist()`. You may need to tweak the `nclass` argument to get the desired number of bins, as the default may not look appropriate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"Hmisc\")\nlibrary(Hmisc)\nhist(loyn, nclass = 20)\n```\n\n::: {.cell-output-display}\n![](template_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n## `ggplot()`\n\nA more modern approach is to use `ggplot()` with `facet_wrap()` to arrange multiple plots on a single page. To do this, the `pivot_longer()` function from the `tidyr` package is used to reshape the data into a tidy format.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# tidy the data\nloyn_tidy <- pivot_longer(loyn, cols = everything())\n\n# plot\nggplot(loyn_tidy, aes(x = value)) + \n  geom_histogram() + \n  facet_wrap(~name, scales = \"free\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](template_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## `ggplot()` with `dplyr`\n\nHere we use the pipe operator `%>%` from `dplyr` to chain together a series of commands. The pipe operator takes the output of the command on the left and passes it to the command on the right (or below) the pipe. This means that we can create a series of commands that are executed in order. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nloyn %>% \n  pivot_longer(cols = everything()) %>% \n  ggplot(aes(x = value)) + \n  geom_histogram() + \n  facet_wrap(~name, scales = \"free\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](template_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n:::\n\n:::{.callout-warning}\n## Question 1\nComment on the histograms in terms of leverage. *Hint: what is the relationship between leverage and skewness?*\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\nThe histograms of `AREA`, `DIST` and `LDIST` are very skewed. The high values would have high leverage, this means that these would cause the residuals to be skewed. These would be candidates for transformation.  \n:::\n\n### Correlation matrix\n\nCalculate the correlation matrix using `cor(Loyn)`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(loyn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              ABUND         AREA      YR.ISOL       DIST       LDIST\nABUND    1.00000000  0.255970206  0.503357741  0.2361125  0.08715258\nAREA     0.25597021  1.000000000 -0.001494192  0.1083429  0.03458035\nYR.ISOL  0.50335774 -0.001494192  1.000000000  0.1132175 -0.08331686\nDIST     0.23611248  0.108342870  0.113217524  1.0000000  0.31717234\nLDIST    0.08715258  0.034580346 -0.083316857  0.3171723  1.00000000\nGRAZE   -0.68251138 -0.310402417 -0.635567104 -0.2558418 -0.02800944\nALT      0.38583617  0.387753885  0.232715406 -0.1101125 -0.30602220\n              GRAZE        ALT\nABUND   -0.68251138  0.3858362\nAREA    -0.31040242  0.3877539\nYR.ISOL -0.63556710  0.2327154\nDIST    -0.25584182 -0.1101125\nLDIST   -0.02800944 -0.3060222\nGRAZE    1.00000000 -0.4071671\nALT     -0.40716705  1.0000000\n```\n:::\n:::\n\n\n:::{.callout-warning}\n## Question 2\nWhich independent variables are useful for predicting the dependent variable abundance? Is there evidence for multi-collinearity? \n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\nSome of the predictors are useful, but AREA has a low r.  \n\nThe correlation between `GRAZE` and `YR.ISOL` is quite high (r = -0.63556710), suggesting multi-collinearity which may influence the model. If the relationship between these two variables was stronger, we would remove one of the variables to prevent this collinearity from affecting the model.\n\n*Note: For more information on collinearity and how it may impact the model, see Quinn & Keough p 127.*\n:::\n\n### Plotting correlation\n\nExamine correlations visually using `pairs()` or `corrplot()` from the `corrplot` package.\n\n:::{.panel-tabset}\n\n## Scatterplot matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(loyn)\n```\n\n::: {.cell-output-display}\n![](template_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n## Correlation matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(corrplot)\ncorrplot(cor(loyn))\n```\n\n::: {.cell-output-display}\n![](template_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\n:::{.callout-warning}\n## Question 3\nAre there any trends visible from the plots?\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\nNot really; the pairs plot reflects the strength of the linear relationship between each of the variables. There may be some stronger relationships occurring, but it is evident a few of the variables are skewed so it is harder to distinguish within the plots.\n:::\n\n\n:::{.callout-tip}\nWe can also bring in variance inflation factors (VIF) to help us identify multi-collinearity, but that is done only after we have selected a model.\n:::\n\n### Transformations\n\nThe AREA predictor has a small number of observations with very large values.  Apply a log~10~ transformation and label the new variable `Loyn$L10AREA`.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nloyn$L10AREA <- log10(loyn$AREA)\n```\n:::\n\n\n:::{.callout-warning}\n## Question 4\nWhy are we transforming AREA?\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\nYou do this to stabilise the variance of the regression to manage the leverage of the outliers in the variable. This reduces the skew.\nL10AREA is more likely to be a significant predictor.  \n:::\n\n:::{.callout-warning}\n## Question 5\nRe-run `pairs(Loyn)` and create a histogram using the transformed value of AREA, how do the plots look?\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(loyn$L10AREA)\npairs(loyn)\n```\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\n- Histogram looks better, less skewed\n- Pairs plot shows a trend between ABUND and L10AREA\n:::\n\n:::{.callout-warning}\n## Question 6\nIn preparation for modelling, transform the remaining skewed variables, DIST and LDIST the same way you did for AREA and examine the histogram and pairs plots using these new variables.  \n:::\n\nMake sure you end up with two new variables labelled `loyn$L10DIST` and `loyn$L10LDIST`. \n\n\n\n\n\n\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\nHistogram for both look better, less skewed\nPairs plot shows potential trend between ABUND and L10DIST, and ABUND with L10LDIST compared to untransformed DIST and LDIST\n:::\n\n\n## Exercise 2: Modelling bird abundance\n\nWe will now use the transformed data in `loyn` for this exercise. If you have not already figured out how to perform the transformation, or if something is wrong, you may use the `loyn` tab in the `mlr.xlsx` MS Excel document. Alternatively, the code to convert the data is below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# reset the data import just in case it has been modified\nloyn <- read_xlsx(\"mlr.xlsx\", \"Loyn\")\n# make transformations\n\nloyn <- loyn %>%\n  mutate(L10AREA = log10(AREA),\n    L10DIST = log10(DIST),\n    L10LDIST = log10(LDIST))\n\n# check\nglimpse(loyn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 56\nColumns: 10\n$ ABUND    <dbl> 5.3, 2.0, 1.5, 17.1, 13.8, 14.1, 3.8, 2.2, 3.3, 3.0, 27.6, 1.…\n$ AREA     <dbl> 0.1, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2…\n$ YR.ISOL  <dbl> 1968, 1920, 1900, 1966, 1918, 1965, 1955, 1920, 1965, 1900, 1…\n$ DIST     <dbl> 39, 234, 104, 66, 246, 234, 467, 284, 156, 311, 66, 93, 39, 4…\n$ LDIST    <dbl> 39, 234, 311, 66, 246, 285, 467, 1829, 156, 571, 332, 93, 39,…\n$ GRAZE    <dbl> 2, 5, 5, 3, 5, 3, 5, 5, 4, 5, 3, 5, 2, 1, 5, 5, 3, 3, 3, 2, 2…\n$ ALT      <dbl> 160, 60, 140, 160, 140, 130, 90, 60, 130, 130, 210, 160, 210,…\n$ L10AREA  <dbl> -1.0000000, -0.3010300, -0.3010300, 0.0000000, 0.0000000, 0.0…\n$ L10DIST  <dbl> 1.591065, 2.369216, 2.017033, 1.819544, 2.390935, 2.369216, 2…\n$ L10LDIST <dbl> 1.591065, 2.369216, 2.492760, 1.819544, 2.390935, 2.454845, 2…\n```\n:::\n:::\n\n\n### Best single predictor?\n\n:::{.callout-warning}\n## Question 1\nObtain the correlation between ABUND and all of the predictor variables using `cor()`.  Based on these, what would you expect to be the best single predictor of ABUND?  \n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(loyn)\n```\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(loyn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               ABUND         AREA      YR.ISOL       DIST       LDIST\nABUND     1.00000000  0.255970206  0.503357741  0.2361125  0.08715258\nAREA      0.25597021  1.000000000 -0.001494192  0.1083429  0.03458035\nYR.ISOL   0.50335774 -0.001494192  1.000000000  0.1132175 -0.08331686\nDIST      0.23611248  0.108342870  0.113217524  1.0000000  0.31717234\nLDIST     0.08715258  0.034580346 -0.083316857  0.3171723  1.00000000\nGRAZE    -0.68251138 -0.310402417 -0.635567104 -0.2558418 -0.02800944\nALT       0.38583617  0.387753885  0.232715406 -0.1101125 -0.30602220\nL10AREA   0.74003580  0.584651024  0.278414517  0.3047850  0.33680642\nL10DIST   0.12672333  0.163054319 -0.019572228  0.8233190  0.29365797\nL10LDIST  0.11812448  0.101607829 -0.161116108  0.4968169  0.82059568\n               GRAZE        ALT    L10AREA     L10DIST    L10LDIST\nABUND    -0.68251138  0.3858362  0.7400358  0.12672333  0.11812448\nAREA     -0.31040242  0.3877539  0.5846510  0.16305432  0.10160783\nYR.ISOL  -0.63556710  0.2327154  0.2784145 -0.01957223 -0.16111611\nDIST     -0.25584182 -0.1101125  0.3047850  0.82331904  0.49681692\nLDIST    -0.02800944 -0.3060222  0.3368064  0.29365797  0.82059568\nGRAZE     1.00000000 -0.4071671 -0.5590886 -0.14263922 -0.03399082\nALT      -0.40716705  1.0000000  0.2751428 -0.21900701 -0.27404380\nL10AREA  -0.55908864  0.2751428  1.0000000  0.30216662  0.38247952\nL10DIST  -0.14263922 -0.2190070  0.3021666  1.00000000  0.60386637\nL10LDIST -0.03399082 -0.2740438  0.3824795  0.60386637  1.00000000\n```\n:::\n:::\n\n\nThe best single predictor would be `L10AREA` as this has the highest *r* (r = 0.74)\n:::\n\n\n### Assumptions and interpretation\n\n:::{.callout-warning}\n## Question 2\nUse multiple linear regression to see whether ABUND can be predicted from L10AREA and GRAZE. Are the assumptions met? Is there a significant relationship? *Note: we are using these 2 predictors as they have the largest absolute correlations. \nUse `lm()` and specify the model as `ABUND ~ L10AREA + GRAZE`.*\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm.mod1 <- lm(ABUND~GRAZE + L10AREA, data=loyn)\n\npar(mfrow=c(2,2))\nplot(lm.mod1)\npar(mfrow=c(1,1))\n\nsummary(lm.mod1)\n```\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm.mod1 <- lm(ABUND~GRAZE + L10AREA, data=loyn)\n\npar(mfrow=c(2,2))\nplot(lm.mod1)\n```\n\n::: {.cell-output-display}\n![](template_files/figure-html/mlmod-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow=c(1,1))\n\nsummary(lm.mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = ABUND ~ GRAZE + L10AREA, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.4296  -4.3186  -0.6323   4.1273  13.0739 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  21.6029     3.0917   6.987 4.73e-09 ***\nGRAZE        -2.8535     0.7125  -4.005 0.000195 ***\nL10AREA       6.8901     1.2900   5.341 1.98e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.444 on 53 degrees of freedom\nMultiple R-squared:  0.6527,\tAdjusted R-squared:  0.6396 \nF-statistic: 49.81 on 2 and 53 DF,  p-value: 6.723e-13\n```\n:::\n:::\n\n\nThis is a significant model as both b1 and b2 are significant and the model is significant.\n\nThe residuals look reasonable. They are approximately normally distributed (both right hand plots), but possibly the variance is not totally constant and there are possibly a few values with high leverage (left hand plots). \n:::\n\n\n:::{.callout-warning}\n## Question 3\nHow good is the model based on the (i) *r*^2^ (ii) adjusted *r*^2^? Use `summary()`.  \n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm.mod1)$r.squared\nsummary(lm.mod1)$adj.r.squared  \n```\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\nThe Adjusted *r*^2^ is lower than the *r*^2^, but we would opt for the adjusted *r*^2^ as it takes the number of predictors into account. Overall the model is ok, explaining 64.0% of variation in Abundance.\n:::\n\n:::{.callout-warning}\n## Question 4\nWhich variable(s) has the most significant effect(s)? *(Refer specifically to the t probabilities in the table of predictors and their estimated parameters or coefficients in the output of `summary()`)*.  Interpret the p-values in terms of dropping predictor variables.  \n:::\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\nBoth `L10AREA` and `GRAZE` are highly significant, `L10AREA` is the most significant. \nIn terms of effect, a 1 unit change in `GRAZE` results in a -2.9 decrease in abundance (with `L10AREA` remaining constant), while a 1 unit change in `L10AREA`, (therefore a 10 unit change in `AREA`) results in a 6.9 increase in abundance (`GRAZE` holding constant). \n:::\n\n:::{.callout-warning}\n## Question 5\nRepeat the multiple regression, but this time include YRS.ISOL as a predictor variable (it has the 3rd largest absolute correlation). This will allow you to assess the effect of YRS.ISOL with the other variables taken into account.  \n:::\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\n\n::: {.cell}\n\n```{.r .cell-code}\nlm.mod2 <- lm(ABUND ~ GRAZE + L10AREA + YR.ISOL, data=loyn)\n```\n:::\n\n:::\n\n:::{.callout-warning}\n## Question 6\nCheck assumptions, do the residuals look ok? If you are happy with the assumptions, you can proceed to interpret the model output.\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\nplot(lm.mod2)\n```\n\n::: {.cell-output-display}\n![](template_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow=c(1,1))\n```\n:::\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n::: {.cell}\n\n:::\n\n:::\n\n:::{.callout-warning}\n## Question 7\nCompare the *r*^2^ and adjusted *r*^2^ values with those you calculated for the 2 predictor model, Which is the better model?  Why?\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm.mod2)\n```\n:::\n\n\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\nBoth of these are greater than for model in step 3, so this is a better model.\n:::\n\n\n## At your own time: California streamflow \n\n:::{.callout-note}\nThis additional exercise can be done at your own time. Most of the code are provided. You will need to run the code and interpret the results.\n:::\n\nThe following dataset contains 43 years of annual precipitation measurements (in mm) taken at (originally) 6 sites in the Owens Valley in California. I have reduced this to three variables labelled `L10APSAB` (Lake Sabrina), `L10OBPC` (Big Pine Creek), `L10OPRC` (Rock Creek), and the dependent variable stream runoff volume (measured in ML/year) at a site near Bishop, California (labelled `L10BSAAM`). There is also a variable `Year` but you can ignore this.\n\nNote the variables have already been log-transformed to increase normality of the residuals in the regressions. \n\nStart with a full model and manually remove the variables one at a time, checking every time whether removal of a variable actually improves the model.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read in the data\ns.data <- read_xlsx(\"mlr.xlsx\", \"California_streamflow\")\nnames(s.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"L10APSAB\" \"L10OBPC\"  \"L10OPRC\"  \"L10BSAAM\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ns.mod_full <-lm(L10BSAAM~L10APSAB + L10OBPC + L10OPRC, data=s.data)\ns.mod_full <-lm(L10BSAAM~., data=s.data) ## you can also use the . to indicate use all variables\nsummary(s.mod_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = L10BSAAM ~ ., data = s.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.09885 -0.03331  0.01025  0.03359  0.09495 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.25716    0.12360  26.352  < 2e-16 ***\nL10APSAB     0.05631    0.03756   1.499  0.14185    \nL10OBPC      0.21085    0.06756   3.121  0.00339 ** \nL10OPRC      0.43838    0.08798   4.983 1.32e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04861 on 39 degrees of freedom\nMultiple R-squared:  0.8817,\tAdjusted R-squared:  0.8726 \nF-statistic: 96.88 on 3 and 39 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n\n### Partial F-Tests\n\nThe above analysis tells us that both `L10OBPC` & `L10OPRC` are significant, according to the t-test, in the model and `L10APSAB` is not?  This involves performing Partial F-Tests as discussed in the lecture.  \n\nThis can be done in **R** by using `anova()` on two model objects. To be able to compare the models and run the anova, you need to make objects of all the possible model combinations you want to compare.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\ns.mod_reduced <- lm(L10BSAAM ~ L10OPRC + L10OBPC, data=s.data)\nanova(s.mod_reduced, s.mod_full)\n```\n:::\n\n\nThe last row gives the results of the partial F-test.  \n\n:::{.callout-warning}\n## Question 1\nShould we remove `L10APSAB` from the model?\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\nYes, we should remove L10APSAB as the p-value is > 0.05 and opt for the simpler model.\n:::\n\n:::{.callout-warning}\n## Question 2\nIs the p-value for the f-test the same as for the t-test? \n:::\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\nYes, P-values for the t-statistic and for the Partial F-statistic are related (Partial F = t^2^)\n:::\n\n:::{.callout-warning}\n## Question 3\nWrite out the hypotheses you are testing.  \n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\n\nH~0~: $\\beta_{L10APSAB} = 0$  \nH~1~: $\\beta_{L10APSAB} \\neq 0$  \n:::\n\nPerform a Partial F-Test to work out if the removal of `L10APSAB` and `L10OBPC` improves upon the full model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns.mod_reduced2  <- lm(L10BSAAM ~ L10APSAB + L10OBPC,data=s.data)\nanova(s.mod_reduced2, s.mod_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: L10BSAAM ~ L10APSAB + L10OBPC\nModel 2: L10BSAAM ~ L10APSAB + L10OBPC + L10OPRC\n  Res.Df      RSS Df Sum of Sq     F    Pr(>F)    \n1     40 0.150845                                 \n2     39 0.092166  1   0.05868 24.83 1.321e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n:::{.callout-warning}\n## Question 4\nWhich variable should be added to the model containing L10OPRC?\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\nL10APSAB does not improve the model with only L10OPRC ($\\beta_{L10APSAB} = 0$), so we can say that we should add L10OBPC to the model containing L10OPRC.   \n\nRemember: H0: No difference between the models, so choose the simplest\n          H1: Full model is better\n:::\n\n\n:::{.callout-warning}\n## Question 5\nCould things be even simpler? Perform a partial F-Test to see if a model containing L10OPRC alone could be suitable.  \n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns.mod_reduced3  <- lm(L10BSAAM ~ L10OPRC,data=s.data)\nanova(s.mod_reduced3, s.mod_full)\n```\n:::\n\n\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\n\nFitting with only L10OPRC does not improve model fit (P<0.05) and so we can conclude that the better model is the one with L10OBPC and L10OPRC as predictors, with L10APSAB removed. \n:::\n\n:::{.callout-warning}\n## Question 6\nWhat is your optimal model?\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\n\nThe best model is: $L10BSAAM = \\beta_0 + \\beta_1 L10OPRC + \\beta_2 L10OBPC + error$\n:::\n\n<hr>\n\nThat's it for today! Great work fitting simple and multiple linear regression! Next week we jump into stepwise selection and predictive modelling! \n\n",
    "supporting": [
      "template_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}