{
  "hash": "c67c40ef6ccb95ae647e0105d5c71385",
  "result": {
    "markdown": "---\ntitle: \"ENVX2001 Lab - Predictive modelling\"\n---\n\n\n\n\n:::{.callout-tip}\nPlease work on this exercise by creating your own R Markdown file.\n:::\n\n## Exercise 1: Backward elimination in R\n\nData:  *Dippers* spreadsheet\n\n\n\n[Dippers](https://en.wikipedia.org/wiki/Dipper) are thrush-sized birds living mainly in the upper reaches of rivers, which feed on benthic invertebrates by probing the river beds with their beaks. The dataset in this exercise contains data from a biological survey which examined the nature of the variables thought to influence the breeding of British dippers. \n\nTwenty-two sites were included in the survey. Some of the variables have been transformed. \n\nThe variables measured were:  \n\n- `Altitude` site altitude\n- `Hardness` water hardness\n- `RiverSlope` river-bed slope\n- `LogCadd` the numbers of caddis fly larvae, transformed\n- `LogStone` the numbers of stonefly larvae, transformed\n- `LogMay` the numbers of mayfly larvae, transformed\n- `LogOther` the numbers of all other invertebrates collected, transformed\n- `Br_Dens` the number of breeding pairs of dippers per 10 km of river  \n\nIn the analyses, the four invertebrate variables were transformed using a $log(x+1)$ transformation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\nDippers <- read_xlsx(\"mlr.xlsx\" , sheet = \"Dippers\")\nglimpse(Dippers)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 22\nColumns: 8\n$ Altitude   <dbl> 259, 198, 251, 184, 145, 145, 198, 160, 251, 159, 160, 145,~\n$ Hardness   <dbl> 12.20, 22.00, 26.30, 22.50, 29.50, 39.90, 42.80, 59.60, 69.~\n$ RiverSlope <dbl> 10.90, 14.70, 6.90, 4.60, 1.91, 5.00, 6.20, 14.30, 4.60, 3.~\n$ Br_Dens    <dbl> 3.60, 4.30, 3.80, 3.40, 3.80, 4.50, 4.30, 5.00, 4.50, 3.40,~\n$ LogCadd    <dbl> 2.303, 2.890, 3.784, 4.419, 3.219, 3.932, 3.664, 4.431, 3.7~\n$ LogStone   <dbl> 5.242, 4.344, 5.231, 5.242, 3.829, 4.898, 4.357, 6.337, 5.4~\n$ LogMay     <dbl> 0.000, 3.401, 5.826, 5.749, 5.509, 5.749, 5.371, 0.000, 4.8~\n$ LogOther   <dbl> 1.386, 1.609, 1.386, 1.386, 1.099, 3.045, 1.386, 2.944, 2.5~\n```\n:::\n:::\n\n\nYou may explore the data on your own (hint: look at last week's exercise on histogram and scatterplot matrices).\n\nWhen ready, perform a backward elimination starting from the full model: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nFullMod <- lm(Br_Dens ~ ., data=Dippers)\nRedMod <- step(FullMod, direction = \"backward\")\nsummary(FullMod)\nsummary(RedMod)\nAIC(FullMod, RedMod)\n```\n:::\n\n\n:::{.callout-warning}\n## Question 1\n\nWhich model is chosen? Why?\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\n\nThe reduced model is chosen as it is better and has the lower AIC. In addition, the adjusted *r*^2^ is higher for the reduced model, indicating that the reduced model is a better fit to the data. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nFullMod <- lm(Br_Dens ~ ., data=Dippers)\nRedMod <- step(FullMod, direction = \"backward\", trace = 0) # backward selection\n\n# Plot as table (a good trick for reports!)\ncompare <- AIC(FullMod, RedMod) %>% # get AIC data frame\n  # then, add adjusted r-squared values in a new column using `mutate()`\n  mutate(adj_r2 = c(summary(FullMod)$adj.r.squared, summary(RedMod)$adj.r.squared)) %>%\n  # then, round all numeric values to 2 decimal places\n  mutate(across(where(is.numeric), \\(x) round(x, 2)))\n\nknitr::kable(compare)\n```\n\n::: {.cell-output-display}\n|        | df|   AIC| adj_r2|\n|:-------|--:|-----:|------:|\n|FullMod |  9| 56.08|   0.76|\n|RedMod  |  7| 53.92|   0.78|\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(FullMod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Br_Dens ~ ., data = Dippers)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24313 -0.38686  0.06925  0.42252  1.19902 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -0.881054   1.215055  -0.725   0.4803  \nAltitude    -0.001801   0.003537  -0.509   0.6186  \nHardness     0.009956   0.004961   2.007   0.0645 .\nRiverSlope   0.080254   0.035781   2.243   0.0416 *\nLogCadd      0.408917   0.274412   1.490   0.1584  \nLogStone     0.604083   0.249207   2.424   0.0295 *\nLogMay       0.114607   0.114116   1.004   0.3323  \nLogOther    -0.267073   0.131828  -2.026   0.0623 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7208 on 14 degrees of freedom\nMultiple R-squared:  0.8433,\tAdjusted R-squared:  0.765 \nF-statistic: 10.77 on 7 and 14 DF,  p-value: 0.0001085\n```\n:::\n\n```{.r .cell-code}\nsummary(RedMod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Br_Dens ~ Hardness + RiverSlope + LogCadd + LogStone + \n    LogOther, data = Dippers)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.18190 -0.49120 -0.08373  0.41721  1.31073 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -0.724619   1.028416  -0.705   0.4912  \nHardness     0.011811   0.004519   2.613   0.0188 *\nRiverSlope   0.065384   0.031545   2.073   0.0547 .\nLogCadd      0.548799   0.226169   2.426   0.0274 *\nLogStone     0.512494   0.229020   2.238   0.0398 *\nLogOther    -0.268128   0.123932  -2.164   0.0460 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7029 on 16 degrees of freedom\nMultiple R-squared:  0.8297,\tAdjusted R-squared:  0.7765 \nF-statistic: 15.59 on 5 and 16 DF,  p-value: 1.169e-05\n```\n:::\n\n```{.r .cell-code}\nAIC(FullMod, RedMod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        df      AIC\nFullMod  9 56.08470\nRedMod   7 53.91655\n```\n:::\n:::\n\n\n:::\n\n\n## Exercise 2: Model quality - multiple vs adjusted *r*^2^ \n\nData:  *California_streamflow* spreadsheet  \n\nImport the \"California_streamflow\" sheet into R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstream <- read_xlsx(\"mlr.xlsx\", \"California_streamflow\")\n```\n:::\n\n\nin this exercise we will use the same data as last week. To jog your memory, the dataset contains 43 years of annual precipitation measurements (in mm) taken at (originally) 6 sites in the Owens Valley in California. Through model selection via partial F-test we have the final model as below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(L10BSAAM ~ L10OPRC + L10OBPC, data = stream)\n```\n:::\n\n\nWe will now add a totally useless variable to the dataset. This variable is a random number generated from a normal distribution with mean 3 and standard deviation 2. We use the `set.seed()` function to make sure that everybody gets the same random values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(100) # to make sure everybody gets the same results\n\n# this generates the random number into the dataset\nstream$random_no <- rnorm(n = nrow(stream), mean = 3, sd = 2) \n```\n:::\n\n  \nWe will see the impact of including a totally useless variable, such as this random variable, has on measures of model quality, *r*^2^ and adjusted *r*^2^ values.\n\n**Task: create two regression models:**\n\n1.  `L10BSAAM ~ L10OPRC + L10OPBC` \n2.  `L10BSAAM ~ L10OPRC + L10OPBC + random_no`  \n\n:::{.callout-warning}\n## Question 2\nCompare each in terms of their multiple *r*^2^ and adjusted *r*^2^ values. Which performance measure (multiple *r*^2^ or adj *r*^2^) would you use to identify which predictors to use in your model?  \n:::\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\n\nThere is only a small difference in the multiple *r*^2^ and adj *r*^2^ between the models, but one goes up and the other goes down. \nSince the random number is a totally useless value, this demonstrates that the adjusted *r*^2^ is the better performance measure to use. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_rand1 <- lm(L10BSAAM ~ L10OPRC + L10OBPC,data = stream)\nsummary(mod_rand1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = L10BSAAM ~ L10OPRC + L10OBPC, data = stream)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.09832 -0.02350  0.01076  0.03291  0.08568 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.35762    0.10547  31.835  < 2e-16 ***\nL10OPRC      0.44437    0.08925   4.979 1.26e-05 ***\nL10OBPC      0.21051    0.06861   3.068  0.00385 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04937 on 40 degrees of freedom\nMultiple R-squared:  0.8749,\tAdjusted R-squared:  0.8686 \nF-statistic: 139.8 on 2 and 40 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nmod_rand2 <- lm(L10BSAAM ~ L10OPRC + L10OBPC + random_no ,data = stream)\nsummary(mod_rand2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = L10BSAAM ~ L10OPRC + L10OBPC + random_no, data = stream)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.10327 -0.02401  0.01220  0.03179  0.08056 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 3.359356   0.106303  31.602  < 2e-16 ***\nL10OPRC     0.442795   0.089956   4.922  1.6e-05 ***\nL10OBPC     0.207234   0.069324   2.989  0.00482 ** \nrandom_no   0.003213   0.005077   0.633  0.53055    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04974 on 39 degrees of freedom\nMultiple R-squared:  0.8761,\tAdjusted R-squared:  0.8666 \nF-statistic: 91.96 on 3 and 39 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n:::\n\n\n## Exercise 3: Making predictions\n\nData:  *California_streamflow* spreadsheet\n\nLet's use the same model again. We will import the data once more just in case it has been modified in the previous exercise.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read in the data\nrequire(readxl)\nstream <- read_xlsx(\"mlr.xlsx\", \"California_streamflow\")\n# best model\nML_Mod2 <- lm(L10BSAAM ~ L10OPRC + L10OBPC, data = stream)\nsummary(ML_Mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = L10BSAAM ~ L10OPRC + L10OBPC, data = stream)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.09832 -0.02350  0.01076  0.03291  0.08568 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.35762    0.10547  31.835  < 2e-16 ***\nL10OPRC      0.44437    0.08925   4.979 1.26e-05 ***\nL10OBPC      0.21051    0.06861   3.068  0.00385 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04937 on 40 degrees of freedom\nMultiple R-squared:  0.8749,\tAdjusted R-squared:  0.8686 \nF-statistic: 139.8 on 2 and 40 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nIf we wish to predict y for a specific pair of values of $x1$ and $x2$, we can simply substitute these into the fitted model:\n\n$$\n\\hat{y} = 3.35762 + 0.44437\\times{L10OPRC} + 0.21051\\times{L10OBPC}\n$$\n\nFor example, if $L10OPRC = 2$ and $L10OBPC = 3$, then $L10BSAAM = \\hat{y} = 4.87789$. \n\nIt is also convention to give a standard error (SE) for any prediction. The formula for the SE of a prediction from a 2 predictor linear regression model is complex (see page 215 of Mead et al, 2003). However, in  R it is simple to also return a corresponding SE value using `predict()` and specifying `se.fit=T`. In this case the output will then include an element called \"se.fit\".\n\nThe tricky bit with `predict()` in R is that you need to specify `newdata` (see the help file), which has to be exactly the same structure as the original data. So to repeat the above example in R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a new data frame with the variables to predict at\n# Note that it does not matter what you put in for L10BSAAM\nnew.df <- data.frame(L10BSAAM = 0, L10OPRC = 2, L10OBPC = 3)\n\n# now use predict() and specify se.fit=T\npredLake <- predict(ML_Mod2, newdata = new.df, se.fit = TRUE)\n\n# the output now has two elements:\n# the fit\npredLake$fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n4.877918 \n```\n:::\n\n```{.r .cell-code}\n# the se of the fit\npredLake$se.fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.07583833\n```\n:::\n:::\n\n\nThis prediction is well within the range original data set.  \n\n:::{.callout-warning}\n## Question 3a\n\nWhy would the prediction be witin the range of of the original dataset? You can use `range()` to figure this out, or just look at the original data.  \n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n## Answer\n\n\n::: {.cell}\nThe se.fit is within the range of the response variable\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredLake$fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n4.877918 \n```\n:::\n\n```{.r .cell-code}\nrange(stream$L10BSAAM)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.712153 5.256511\n```\n:::\n:::\n\n\n:::\n\nMore interesting is making prediction not part of the original data, but as we discussed in the lectures this means there is a different confidence interval. \n\nR allows you to define the interval using `interval = \"prediction\"`. The output will then include both the fitted and the prediction confidence interval and the default is to calculate the 95% confidence interval. \n\nIf you want to calculate the actual se.fit from the output, you need to subtract the actual prediction and divide by the t~0.05~ for `df = 40` (which is n - p - 1). \n\nCreate a new data frame with the variables to predict. Note that it again does not matter what you put in for the response (dependent) L10BSAAM, we have put in five 0 values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew.df <- data.frame(L10OPRC = seq(3.0, 4.0, length = 5),\n                     L10OBPC = seq(3.0, 4.0, length = 5))\n```\n:::\n\n\n\n:::{.callout-warning}\n## Question 3b\n\nNow use `predict()` and specify `interval=\"prediction\"`.  \n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLake_predict <- predict(ML_Mod2, newdata=new.df, interval=\"prediction\")\nLake_predict\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 5.322293 5.210502 5.434085\n2 5.486016 5.363304 5.608728\n3 5.649738 5.513492 5.785984\n4 5.813460 5.661764 5.965157\n5 5.977183 5.808647 6.145719\n```\n:::\n:::\n\n\n:::\n\nInspect the output; the `lwr` and `upr` columns are the upper and lower prediction intervals. Note that the variation and prediction intervals are fairly small.  \n\n\n\n## Exercise 4: Validation and model prediction quality\n\nData:  *California Streamflow* spreadsheet  \n\nIn this exercise we will test the quality of the developed model, but doing it formally using a comparison on a validation data set.\n\nWe have to once again `set.seed()` to make sure your results are the same across the class.\n\nThe first step is to sample 20% of the data as a validation data set from the overall data set. We are doing this by using the function `sample()` to pick a random number of rows. We can use `dim()` to check the dimensions of the datasets.     \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Only use so we are all get same random numbers - \n# otherwise R uses computer time to get random number\nset.seed(10)\n\n#Sample 20% of the rows, find row numbers\nindex <- sample(1:nrow(stream), size = 0.20*nrow(stream))\n#Split data\nvalid <- stream[index,]\ndim(valid)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8 4\n```\n:::\n\n```{.r .cell-code}\ncalib <- stream[-index,]\ndim(calib)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 35  4\n```\n:::\n:::\n\n\nRather than rerunning the calibration we are going to reuse the two models from last week with different data and compare the results. We will use the model with 2 variables and the model with 3 variables.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# use model 2 and model 3 from topic 7/8 practical (last week)\n# and test which one is the best model, but use calib data\nML_Mod2 <- lm(L10BSAAM ~ L10OPRC + L10OBPC, data = calib)\nML_Mod3 <- lm(L10BSAAM ~ L10OPRC + L10OBPC + L10APSAB, data = calib)\n\n# compare the models\nsummary(ML_Mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = L10BSAAM ~ L10OPRC + L10OBPC, data = calib)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.10393 -0.02396  0.01011  0.03323  0.08423 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.35322    0.11734  28.577  < 2e-16 ***\nL10OPRC      0.42263    0.09872   4.281 0.000158 ***\nL10OBPC      0.23667    0.07411   3.193 0.003150 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05016 on 32 degrees of freedom\nMultiple R-squared:  0.8852,\tAdjusted R-squared:  0.878 \nF-statistic: 123.3 on 2 and 32 DF,  p-value: 9.154e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(ML_Mod3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = L10BSAAM ~ L10OPRC + L10OBPC + L10APSAB, data = calib)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.10398 -0.02560  0.00741  0.02820  0.09170 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.25540    0.13977  23.291  < 2e-16 ***\nL10OPRC      0.42590    0.09786   4.352 0.000136 ***\nL10OBPC      0.22977    0.07364   3.120 0.003892 ** \nL10APSAB     0.05279    0.04187   1.261 0.216703    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0497 on 31 degrees of freedom\nMultiple R-squared:  0.8908,\tAdjusted R-squared:  0.8802 \nF-statistic: 84.26 on 3 and 31 DF,  p-value: 5.361e-15\n```\n:::\n:::\n\n\nBased on the results, ML_Mod3 is better based on a slightly better adj *r*^2^. Because the difference is so slight, should do a more thorough investigation which model is better.\n\nYou might want to check the residual plots of the predictions. Do you observe anything suspicious?  \n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\nplot(ML_Mod2)\n```\n\n::: {.cell-output-display}\n![Residual plots for Model with 2 parameters](ENVX2001-2024-Lab09_files/figure-pdf/ResidualMod2-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\npar(mfrow=c(1,1))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\nplot(ML_Mod3)\n```\n\n::: {.cell-output-display}\n![Residual plots for Model with 3 parameters](ENVX2001-2024-Lab09_files/figure-pdf/ResidualMod3-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\npar(mfrow=c(1,1))\n```\n:::\n\n\n:::{.callout-warning}\n## Question 4\nAccuracy: check RMSE and bias of the calibrated models: \n\nUse the equation for RMSE: \n\n$$ RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2} $$\nand for bias (Mean Error):\n\n$$ ME = \\frac{1}{n}\\sum_{i=1}^{n}y_i - \\hat{y}_i $$\n\nCheck both the models and for both calibration and validation. To derive the validation data, use (for example for Model 2):\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(ML_Mod2, newdata = valid)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1        2        3        4        5        6        7        8 \n4.803687 5.129393 4.918709 4.859044 4.924527 4.911822 4.937253 5.064019 \n```\n:::\n:::\n\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\n\n\n::: {.cell}\n**ANSWER**\n  \nBias and RMSE are low and seem to be more optimal for ML_Mod3\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Accuracy (RMSE)\n# Calibrations\n(RMSE2 <- sqrt(mean((calib$L10BSAAM - predict(ML_Mod2))^2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0479611\n```\n:::\n\n```{.r .cell-code}\n(RMSE3 <- sqrt(mean((calib$L10BSAAM - predict(ML_Mod3))^2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04677635\n```\n:::\n\n```{.r .cell-code}\n# now validations\n(RMSE2_valid <- sqrt(mean((valid$L10BSAAM - predict(ML_Mod2, newdata = valid))^2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04899388\n```\n:::\n\n```{.r .cell-code}\n(RMSE3_valid <- sqrt(mean((valid$L10BSAAM - predict(ML_Mod3, newdata = valid))^2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04734575\n```\n:::\n\n```{.r .cell-code}\n#Bias\n(Bias2 <- mean(calib$L10BSAAM - predict(ML_Mod2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.635181e-16\n```\n:::\n\n```{.r .cell-code}\n(Bias3 <- mean(calib$L10BSAAM - predict(ML_Mod3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.807278e-16\n```\n:::\n\n```{.r .cell-code}\n# very small, as they should be!\n\n(Bias2_valid <- mean(valid$L10BSAAM - predict(ML_Mod2, newdata = valid)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.0346165\n```\n:::\n\n```{.r .cell-code}\n(Bias3_valid <- mean(valid$L10BSAAM - predict(ML_Mod3, newdata = valid)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.03694093\n```\n:::\n:::\n\n\n:::\n\nWe can subsequently make a plot of the calibration and validation data sets for both observed data and the predicted data and compare to the 1:1 line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot predicted versus observed\npar(mfrow = c(2,1), mar=c(4,4,3,2))\n\n# model 2\nplot(calib$L10BSAAM, predict(ML_Mod2), \n     # colour = red, type = \"16\" and size is 20% larger\n     pch = 16, col = \"blue\", cex = 1.2,\n     # add titles for axes and main\n     xlab = \"Observed dataset\", ylab = \"Predicted\",\n     main=\"model 2 predictors\")\n# insert a 1:1 line, dashed line, width = 2\nabline(0, 1, lty = 2, lwd = 2)\n\n# add the validation data\npoints(valid$L10BSAAM, predict(ML_Mod2, newdata = valid), \n       # colour = blue, type = \"16\" and size is 20% larger\n       col = \"red\", pch = 16, cex = 1.2)\n# add a legend to the first plot\nlegend(\"topleft\", c(\"calibration\", \"validation\", \"1 : 1 line\"), \n       pch = c(16, 16, NA), lty = c(NA, NA, 2), col = c(\"blue\", \"red\", 1),\n       # 20% smaller\n       cex=0.8)\n\n# model 3\nplot(calib$L10BSAAM, predict(ML_Mod3), \n     # colour = red, type = \"16\" and size is 20% larger\n     pch = 16, col = \"blue\", cex = 1.2,\n     # add titles for axes and main\n     xlab = \"Observed dataset\", ylab = \"Predicted\",\n     main=\"model 3 predictors\")\n# insert a 1:1 line, dashed line, width = 2\nabline(0, 1, lty = 2, lwd = 2)\n# add the validation data\npoints(valid$L10BSAAM, predict(ML_Mod3, newdata = valid), \n       # colour = blue, type = \"16\" and size is 20% larger\n       col = \"red\", pch = 16, cex = 1.2)\n```\n\n::: {.cell-output-display}\n![Plots of predicted versus observed for model with 2 predictors (top) and model with 3 predictors (bottom)](ENVX2001-2024-Lab09_files/figure-pdf/PlotObsPred-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThis gives the opportunity for a visual inspection, but we can also calculate the correlation (to echo the model derivation) for the calibration and validation data sets. You can also calculate the multiple r^2^ by squaring the correlation coefficient. In this case the multiple r^2^ is ok to use as we are using it to assess prediction quality rather than the initial model fit.  \n\n:::{.callout-warning}\n## Question 4b\n\nCalculate correlation using and r^2^ using `cor()`\n\n:::\n\n\n::: {.content-visible when-profile=\"solution\"}\n\n## Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Correlation\ncor(calib$L10BSAAM, predict(ML_Mod2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9408282\n```\n:::\n\n```{.r .cell-code}\ncor(calib$L10BSAAM, predict(ML_Mod3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9438016\n```\n:::\n\n```{.r .cell-code}\n# we already know this from summary()\n\ncor(valid$L10BSAAM, predict(ML_Mod2, newdata = valid))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9449247\n```\n:::\n\n```{.r .cell-code}\ncor(valid$L10BSAAM, predict(ML_Mod3, newdata = valid))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9670033\n```\n:::\n\n```{.r .cell-code}\n# confirms bias and RMSE calculations\n\n# r2\n(cor(calib$L10BSAAM, predict(ML_Mod2))^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8851578\n```\n:::\n\n```{.r .cell-code}\n(cor(calib$L10BSAAM, predict(ML_Mod3))^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8907614\n```\n:::\n\n```{.r .cell-code}\n(cor(valid$L10BSAAM, predict(ML_Mod2, newdata = valid)))^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8928826\n```\n:::\n\n```{.r .cell-code}\n(cor(valid$L10BSAAM, predict(ML_Mod3, newdata = valid)))^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9350953\n```\n:::\n:::\n\n\n:::\n\n:::{.callout-warning}\n## Question 4c\n\nNow have a go at calculating Lin's concordance correlation coefficient, using the lecture slides. \n\nDon't forget to call `library(epiR)` and maybe `install.packages(\"epiR\")` if the package is not installed.\n\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n\n## Answer\n\n\n::: {.cell warnings='false'}\n\n```{.r .cell-code}\n#if needed\n#install.packages(\"epiR\")\nlibrary(epiR, quietly = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage epiR 2.0.73 is loaded\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType help(epi.about) for summary information\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType browseVignettes(package = 'epiR') to learn how to use epiR for applied epidemiological analyses\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n\n```{.r .cell-code}\ncalib.lcc<-epi.ccc(calib$L10BSAAM, predict(ML_Mod2))\ncalib.lcc$rho.c\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        est     lower     upper\n1 0.9390808 0.8841411 0.9684053\n```\n:::\n\n```{.r .cell-code}\nvalid.lcc<-epi.ccc(valid$L10BSAAM, predict(ML_Mod2, newdata = valid))\nvalid.lcc$rho.c\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        est     lower     upper\n1 0.8620478 0.5697695 0.9607329\n```\n:::\n\n```{.r .cell-code}\ncalib.lcc<-epi.ccc(calib$L10BSAAM, predict(ML_Mod3))\ncalib.lcc$rho.c\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        est     lower     upper\n1 0.9422251 0.8898999 0.9700764\n```\n:::\n\n```{.r .cell-code}\nvalid.lcc<-epi.ccc(valid$L10BSAAM, predict(ML_Mod3, newdata = valid))\nvalid.lcc$rho.c\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        est     lower    upper\n1 0.8732522 0.6239647 0.961224\n```\n:::\n:::\n\n\n:::\n\n\n:::{.callout-warning}\n## Question 4d\n\nDraw conclusions about which model is the best model to predict L10BSAAM from the other variables, use the results to support your argument.\n:::\n\n::: {.content-visible when-profile=\"solution\"}\n## Answer\n\n\n::: {.cell}\n\nHere we are looking for a summary of your findings. We would say something like:\n\nEvident from a slightly higher adjusted *r*^2^ (*r*^2^-adj = 0.878 for ML_Mod2, 0.880 for ML_Mod3), ML_Mod3 is a more suitable model to predict variable stream runoff volume at the selected site near Bishop, California (L10BSAAM; measured in ML/year). \n\nTo support our conclusion, we investigated further by comparing accuracy between the two models, in terms of RMSE, Bias, and LCCC. Bias and RMSE are low and seem to be more optimal for ML_Mod3 (Bias = -0.037 ML/year and RMSE = 0.047 ML/year). Furthermore, the LCCC is higher/closer to 1 for ML_Mod3 compared to ML_Mod2 (LCCC = 0.873, LCCC = 0.862, for ML_Mod3 and ML_Mod2, respectively). \n\n**Note:** \n1. Bias and RMSE are in the same units as the response variable, so we provide units when reporting the values.\n2. We have mainly reported the validation statistics in the summary; however we still need calibration statistics so we can compare the two (e.g. RMSE from calib vs RMSE from valid) and assess how well the model predicts, and identify whether there is overfitting. If the model is overfitting, we would see a difference between the model fit statistics between calib and valid datasets. In general, model fit will usually be lower in the predictions. \n\n**Model 2: L10BSAAM ~ L10OPRC + L10OBPC**\n\nModel fit :\n\nBias (calib) = 5.075315e-16\n\nBias (valid) = -0.0346165\n\nRMSE (calib) = 0.0479611\n\nRMSE(valid) = 0.04899388\n\nCalib LCCC= 0.9390808\n\nValid LCCC = 0.8620478\t\n\n**Model 3:L10BSAAM ~ L10OPRC + L10OBPC + L10APSAB**\n\nModel fit:\n  \nBias (calib) = -4.060266e-16\n\nBias (valid) = -0.03694093\n\nRMSE (calib) = 0.04677635\n\nRMSE(valid) = 0.04734575\n\nCalib LCCC = 0.9422251\n\nValid LCCC = 0.8732522\n:::\n\n:::\n\n\n<hr>\n\nThat's it for today! Great work fitting multiple linear regression and trying your hand at some predictive modelling! Next week we jump into stepwise selection and predictive modelling! \n\n",
    "supporting": [
      "ENVX2001-2024-Lab09_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}